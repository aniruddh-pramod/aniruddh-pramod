{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniruddh-pramod/aniruddh-pramod/blob/main/RL_solved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BiU-x0ujcRkE"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) 2020 Brain and Cognitive Society, IIT Kanpur [ BCS @IITK ]\n",
        "# Copyright under MIT License, must reference https://github.com/bcs-iitk/BCS_Workshop_Apr_20 if used anywhere else.\n",
        "# Author: Shashi Kant (http://shashikg.github.io/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0QMGbtcQKNx"
      },
      "source": [
        "## Reinforcement Learning\n",
        "In this you have to implement and train an RL agent to find a path for a frozen lake problem. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH7ubidoNaEv"
      },
      "source": [
        "### Frozen Lake Problem Description:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTCY_Ip1M9Qc"
      },
      "source": [
        "> Imagine there is a frozen lake stretching from your home to your office; you have to walk on the frozen lake to reach your office. But oops! There are holes in the frozen lake so you have to be careful while walking on the frozen lake to avoid getting trapped in the holes. [[src](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781788836524/3/ch03lvl1sec32/solving-the-frozen-lake-problem)]\n",
        "\n",
        "![frozen-lake](https://static.packt-cdn.com/products/9781788836524/graphics/49f3e058-2f32-40e8-9992-b53d1f57d138.png)\n",
        "\n",
        "\n",
        "The task you have to do here:\n",
        "\n",
        "*  Use the Gym library from OpenAI to setup a frozen lake environment and work till around 2000 time steps. Then finally output the Q-Table .\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "env = gym.make('FrozenLake-v0')\n",
        "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
        "\n",
        "eta = 0.628\n",
        "gma = 0.9\n",
        "num_epi = 0\n",
        "tot_time = 0\n",
        "while (tot_time<2100):\n",
        "    obv = env.reset()\n",
        "    d = False\n",
        "    t=0\n",
        "    while not d:\n",
        "        t+=1\n",
        "        #env.render()\n",
        "        #print(observation)\n",
        "        action = np.argmax(Q[obv,:] + np.random.randn(1,env.action_space.n)*(1./(num_epi+1)))\n",
        "        #Get new state & reward from environment\n",
        "        obv_new,reward,done,_ = env.step(action)\n",
        "        #Update Q-Table with new knowledge\n",
        "        Q[obv,action] = Q[obv,action] + eta*(reward + gma*np.max(Q[obv_new,:]) - Q[obv,action])\n",
        "        obv = obv_new\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            tot_time += t+1\n",
        "            break\n",
        "    num_epi += 1\n",
        "env.close()\n",
        "print(\"Final Q Table\")\n",
        "print(Q)"
      ],
      "metadata": {
        "id": "_V99mjAPZKZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3239cd-5426-4f30-80fe-fe0061a1bb03"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Q Table\n",
            "[[6.62989974e-03 5.54308863e-03 3.22210644e-03 6.49932521e-03]\n",
            " [0.00000000e+00 1.74172143e-03 2.44909549e-03 3.18073238e-03]\n",
            " [1.14635593e-03 1.21238238e-03 5.21884474e-03 1.04616275e-03]\n",
            " [1.82672153e-03 1.63860904e-03 0.00000000e+00 3.10414938e-03]\n",
            " [7.36690377e-03 3.02270208e-03 9.81465651e-04 3.09524603e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 4.93289700e-03 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.66616768e-03 3.25703258e-04 8.40162619e-05 8.10970507e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.47204784e-03]\n",
            " [1.50691797e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00647365e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 8.90761773e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Agent Performance\n",
        "total_time = 0\n",
        "total_penalty = 0\n",
        "episodes = 100\n",
        "for _ in range(episodes):\n",
        "  state = env.reset()\n",
        "  time = 0\n",
        "  penalty = 0\n",
        "  reward = 0\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = np.argmax(Q[state])\n",
        "    state, reward, done, info = env.step(action)\n",
        "    if reward<0:\n",
        "      penalty += 1\n",
        "    time += 1\n",
        "  total_time += time\n",
        "  total_penalty += penalty\n",
        "print(f\"Results aftetr {episodes} episodes\")\n",
        "print(f\"Average penalty per run: {total_penalty/episodes}\")\n",
        "print(f\"Average time per run: {total_time/episodes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGh9bGto0J79",
        "outputId": "7c6a7134-bbb6-4ef4-e742-903906f52c81"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results aftetr 100 episodes\n",
            "Average penalty per run: 0.0\n",
            "Average time per run: 26.3\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of RL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}